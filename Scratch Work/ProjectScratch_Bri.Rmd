---
title: "MA641 Project Scratch"
author: "Brianna Cirillo"
output: html_notebook
---

### Libraries

```{r}
# Load necessary libraries
library(forecast)
library(tseries)
library(tidyverse)
```
 

## FRED Monthly Retail Sales
### Load & Inspect the Data

```{r}
# Load in the data
data <- read.csv("~/Documents/GitHub/MA-641-Course-Project/FRED_monthly_retail_sales.csv",header=T)

# Convert the date column to Date type
data$DATE <- as.Date(data$DATE, format="%Y-%m-%d")

# Inspect the data
head(data)
summary(data)
```

- The dataset spans from January 2018 to May 2024, covering over six years of monthly data.
- The retail sales values range from 376,261 to 668,957.
- The median and mean values are close to each other (around 518,800), suggesting a symmetric distribution.
- The interquartile range (IQR) is the difference between the 3rd and 1st quartiles (592,577 - 455,157 = 137,420), which measures the spread of the middle 50% of the data.
- There is a noticeable increase in retail sales over time, as indicated by the higher values in the upper quartiles compared to the lower quartiles.


### Create a Time Series Object

```{r}
# Create a time series object
ts_data <- ts(data$RSXFSN, start=c(2018, 12), frequency=12)

```


### Descriptive Analysis

```{r}
# Descriptive Analysis
plot(ts_data, main="Monthly Retail Sales", ylab="Sales", xlab="Time")
summary(ts_data)
boxplot(ts_data ~ cycle(ts_data), main="Seasonal Boxplot of Monthly Retail Sales", ylab="Sales")

```


Time Series Plot:


- There is an upward trend in retail sales over the period from 2018 to 2024. This indicates that retail sales have been generally increasing over time.
- There are repeating patterns within each year, indicative of seasonality.
- The amplitude of fluctuations increases over time, indicating growing volatility in sales.

Seasonal Boxplots:


- There is evident seasonal variation in monthly retail sales. For example, January (month 1) tends to have lower sales, while November (month 11) shows higher sales.
- The interquartile range (IQR) and the median values vary significantly across months, indicating changes in sales patterns throughout the year.
- The presence of outliers can indicate unusual sales activity in certain months.
- November has the highest median sales, indicating consistently higher sales in this month.


### ACF and PACF Plot

```{r}
# ACF and PACF Plots
par(mar=c(5, 5, 4, 2) + 0.1)
acf(ts_data, main="ACF of Monthly Retail Sales")
pacf(ts_data, main="PACF of Monthly Retail Sales")
```


ACF:


- The ACF plot shows significant autocorrelation at multiple lags, with the autocorrelation values gradually decreasing. This is indicative of a trend component in the data.
- The significant autocorrelations at the first few lags suggest that the past values have a strong influence on the current value of the series.
- The slow decay of the ACF values is typical of non-stationary time series data, where the mean and variance are not constant over time. This pattern suggests that the time series might benefit from differencing to achieve stationarity.
- There is also a noticeable seasonal pattern, as indicated by the peaks at regular intervals (around lag 12 and multiples), which is consistent with the monthly data having a yearly seasonal component.

PACF:


- The PACF plot shows a significant spike at lag 1, with values dropping off sharply afterward. This indicates that an AR(1) component might be appropriate for the model.
- The sharp drop after the first lag in the PACF plot is typical of an autoregressive process, suggesting that the AR component is sufficient to explain the dependencies in the series.
- There is also a smaller spike at lag 12, which supports the presence of seasonality in the data.


### Augmented Dickey-Fuller Test

```{r}
# Augmented Dickey-Fuller Test
adf_test <- adf.test(ts_data, alternative="stationary")
print(adf_test)
```


- The p-value is 0.3911, which is greater than the significance level of 0.05. This means that we fail to reject the null hypothesis at these levels.
- We do not have enough evidence to reject the null hypothesis of non-stationarity. Therefore, we conclude that the time series ts_data is non-stationary.
- Since the time series is non-stationary, we need to make it stationary for modeling purposes. 



```{r}
# Differencing the series if it is not stationary
if (adf_test$p.value > 0.05) {
  ts_data_diff <- diff(ts_data, differences=1)
  adf_test_diff <- adf.test(ts_data_diff, alternative="stationary")
  print(adf_test_diff)
  
  # Update the time series data to the differenced series
  ts_data <- ts_data_diff
}
```

- The p-value is 0.01, which means that we reject the null hypothesis at these levels. Therefore, we conclude that the differenced time series ts_data_diff is stationary.

### MA Model

```{r}
# Fit an MA model
ma_model <- Arima(ts_data, order=c(0,0,1))
summary(ma_model)
```


- The ma1 coefficient is significant, indicating that the moving average term is meaningful. 
- ME close to zero, and reasonably low RMSE, MAE, and MAPE values suggest that the model forecasts well, though the high RMSE might be a concern depending on the context. 
- The ACF1 value suggests some autocorrelation in residuals, implying that there might be room for improvement. 


Overall: The MA(1) model appears to be a reasonable fit for the data, capturing the main patterns and trends. However, the presence of some autocorrelation in the residuals and the relatively high RMSE indicate that it might not be the best possible model. 


### AR Model

```{r}
# Fit an AR model
ar_model <- Arima(ts_data, order=c(1,0,0))
summary(ar_model)
```


- The ar1 coefficient is significant, with a high value indicating a strong autoregressive component.
- AIC, AICc, and BIC values are lower than those for the MA(1) model, suggesting a better fit.
- Lower RMSE, MAE, and MAPE compared to the MA(1) model indicate better predictive performance.
- The ACF1 value is closer to zero, indicating less autocorrelation in residuals.

Overall: The AR(1) model appears to be a better fit than the MA(1) model for this dataset. It captures the main patterns and trends more accurately, with lower error measures and better information criteria values. However, the prediction intervals suggest some caution in the confidence of future values. 


### ARMA Models

```{r}
# Fit an ARMA(1,1) model as a starting point
arma_model <- Arima(ts_data, order=c(1,0,1))
summary(arma_model)
```


The ARMA(1,1) model appears to be a good fit for the data, capturing the main patterns and trends accurately. It provides a better fit compared to the MA(1) model and is comparable to or slightly better than the AR(1) model, especially considering the lower information criteria values and the low autocorrelation of residuals. Further comparison with other ARMA configurations or additional model validation could provide more assurance, but overall, this model seems robust.


```{r}
# Fit ARIMA model
sarima_model <- auto.arima(ts_data)
summary(sarima_model)
```

Model: ARIMA(0,0,0)(2,1,0)[12] with drift


This indicates a SARIMA model with:


- No non-seasonal autoregressive (AR) terms (p=0)
- No differencing (d=0)
- No non-seasonal moving average term (MA=0)
- Seasonal autoregressive terms of order 2 (SAR(2))
- Seasonal differencing of order 1 (D=1)
- No seasonal moving average term (SMA=0)
- Seasonal period of 12 (monthly data)
- A drift term

- The coefficients for the AR and seasonal AR terms are significant, suggesting that the seasonal components at lags 12 and 24 months play a crucial role in explaining the variations in the data.
- AIC, AICc, and BIC values are much lower than those for the previous models, suggesting a significantly better fit.
- RMSE, MAE, and MAPE values are much lower than those of the previous models, indicating excellent predictive performance.
- The ACF1 value close to zero indicates minimal autocorrelation in residuals, suggesting a good fit.
- The intervals are narrower compared to previous models, indicating higher confidence in the forecasts.
Overall: The SARIMA(0,0,0)(2,1,0)[12] with drift model appears to captures the main patterns and trends accurately, with significantly lower error measures and better information criteria values compared to the MA(1), AR(1), and ARMA(1,1) models. The narrow prediction intervals further suggest high confidence in the future forecasts. This model is robust and well-suited for the time series data.

### ACF, PACF, EACF Plots for SARIMA

```{r}
# Plot Residuals of the SARIMA Model
acf_residuals <- residuals(sarima_model)
par(mar=c(5, 5, 4, 2) + 0.1)
acf(acf_residuals, main="ACF of SARIMA Model Residuals")
pacf(acf_residuals, main="PACF of SARIMA Model Residuals")
eacf(acf_residuals)
```


- Most autocorrelations and partial autocorrelations are within the blue dashed significance bounds, indicating that the residuals do not exhibit significant autocorrelation. There are no strong patterns or significant spikes outside the bounds, suggesting that the residuals are approximately white noise.
- This plot suggests that a model with low AR and MA orders may be appropriate, given the clean separation of significant and non-significant correlations.
- The model has successfully captured the main structure of the data, including trends and seasonality.
- The EACF plot supports the model selection by identifying the appropriate AR and MA orders. It provides additional confirmation of the model's adequacy.


### Forecasting

```{r}
# Forecasting with the SARIMA model
sarima_forecast <- forecast(sarima_model, h=12)
plot(sarima_forecast, main="Forecasts from SARIMA(0,0,0)(2,1,0)[12] with Drift")
```

- The forecast for the next 12 months indicates a steady increase in monthly retail sales, with prediction intervals providing a range of expected values. This information can be useful for planning and decision-making purposes in the retail sector.



### Model Comparison

```{r}
# Comparing Models using AIC and BIC
models <- list(ar_model, ma_model, arma_model, sarima_model)
model_names <- c("AR(1)", "MA(1)", "ARMA(1,1)", "SARIMA(0,0,0)(2,1,0)[12] with Drift")

aic_values <- sapply(models, AIC)
bic_values <- sapply(models, BIC)

comparison <- data.frame(Model=model_names, AIC=aic_values, BIC=bic_values)
print(comparison)
```

- The SARIMA(0,0,0)(2,1,0)[12] with Drift model was chosen as the best model based on AIC and BIC values. The model captures both the seasonal and non-seasonal components of the data effectively, and the residuals appear to be white noise, indicating a good fit.


## AAPL Monthly Data
### Load & Inspect the Data

```{r}
# Load the data
data <- read.csv("~/Documents/GitHub/MA-641-Course-Project/AAPL_monthly.csv")

# Convert the date column to Date type
data$Date <- as.Date(data$Date, format="%Y-%m-%d")

# Inspect the data
head(data)
summary(data)
```


### Create a Time Series Object

```{r}
# Create a time series object
ts_data <- ts(data$Close, start=c(2018, 01), frequency=12) 
```


### Descripvtive Analysis

```{r}
# Descriptive Analysis
plot(ts_data, main="Monthly Apple Stock Prices", ylab="Close Price", xlab="Time")
summary(ts_data)
boxplot(ts_data ~ cycle(ts_data), main="Seasonal Boxplot of Monthly Apple Stock Prices", ylab="Close Price")
```


Time Series Plot:

- There is a clear upward trend in Apple stock prices over the period. The prices show a substantial increase, particularly starting around 2020.
- There is visible volatility in the stock prices, with fluctuations becoming more pronounced in the later years.
- The increased volatility may imply higher risk for investors, as the stock prices have larger swings.

Summary:

- The mean and median values suggest that the central tendency of the stock prices is around 115 to 130.
- The range indicates that the stock price has varied significantly over the period.
- The interquartile range (IQR = Q3 - Q1) shows the spread of the middle 50% of the data, which is a useful measure of variability.

Seasonal Boxplot:

- The presence of seasonality suggests that certain months tend to have higher or lower stock prices consistently, which can be crucial for seasonal trading strategies.
- The seasonal boxplot reveals monthly patterns and variability, suggesting that seasonality should be considered in trading strategies and risk 


### ACF & PACF Plots

```{r}
# ACF and PACF Plots
par(mar=c(5, 5, 4, 2) + 0.1)
acf(ts_data, main="ACF of Monthly Apple Stock Prices")
pacf(ts_data, main="PACF of Monthly Apple Stock Prices")
eacf(ts_data)
```


ACF Plot:

- The gradual decay in the ACF indicates the presence of a trend component in the time series. The series is likely non-stationary.
- Significant autocorrelations suggest that the data is not random and past values can help predict future values.
- A high degree of positive autocorrelation at the first few lags implies momentum in the stock prices, which is common in financial time series.


PACF Plot:

- The sharp drop after the first lag in the PACF suggests that an AR(1) model may be appropriate for capturing the relationship in the data.
- The significant first lag indicates that the immediate past value has a strong influence on the current value, while the influence of values further in the past diminishes quickly.
- This pattern supports the use of a simple autoregressive model, as the complexity beyond the first lag does not add much explanatory power.


### ADF Test

```{r}
# Augmented Dickey-Fuller Test
adf_test <- adf.test(ts_data, alternative="stationary")
print(adf_test)
```


- Since the p-value is greater than 0.05, we fail to reject the null hypothesis that the time series has a unit root. This indicates that the series is non-stationary.
- The non-stationarity observed from the ADF test results implies that differencing the time series is necessary to achieve stationarity.


```{r}
# Differencing the series if it is not stationary
if (adf_test$p.value > 0.05) {
  ts_data_diff <- diff(ts_data, differences=1)
  adf_test_diff <- adf.test(ts_data_diff, alternative="stationary")
  print(adf_test_diff)
  
  # Update the time series data to the differenced series
  ts_data <- ts_data_diff
}
```


- Since the p-value is less than 0.05, we reject the null hypothesis that the time series has a unit root. This indicates that the differenced series is stationary.
- With the differenced series being stationary, it is now suitable for fitting ARIMA models.
 

### Fit AR, MA, and ARMA Models
#### AR Model
```{r}
# Fit AR model
ar_model <- Arima(ts_data, order=c(1,0,0))
summary(ar_model)
```


- The AR(1) coefficient is -0.0337, indicating a weak negative relationship between the current and previous value in the time series. This suggests that the current month's stock price is slightly negatively correlated with the previous month's price.
- The mean value of 0.4415 indicates the average level around which the time series fluctuates.
- The standard errors for the coefficients are relatively small, suggesting that the estimates are reasonably precise.
- While the error measures such as RMSE and MAE are low, indicating good prediction accuracy, the high MAPE value suggests potential issues with percentage errors possibly due to the inherent volatility in stock prices.


#### MA Model
```{r}
# Fit MA model
ma_model <- Arima(ts_data, order=c(0,0,1))
summary(ma_model)

```


- The ARIMA(0,0,1) model fits the data with a weak negative moving average component.
- While the error measures such as RMSE and MAE are low, indicating good prediction accuracy, the high MAPE value suggests potential issues with percentage errors possibly due to the inherent volatility in stock prices.
- THE AIC and BIC values are the same for the AR and MA models

#### ARMA Model
```{r}
# Fit ARMA model
arma_model1 <- Arima(ts_data, order=c(1,0,1))
summary(arma_model1)
```


- The ARIMA(1,0,1) model provides a slightly more complex representation by combining AR and MA components. However, the improvement in model fit compared to the simpler ARIMA(0,0,1) model is minimal, as indicated by similar log likelihood, AIC, and BIC values. 
- The high standard errors for the AR and MA coefficients suggest potential issues with estimate precision. 
- Overall, the ARIMA(0,0,1) model (MA model) might be preferred for its simplicity and comparable fit.


```{r}
# ARMA(2,1) Model
arma_model2 <- Arima(ts_data, order=c(2,0,1))
summary(arma_model2)
```


- The ARIMA(2,0,1) model introduces additional AR and MA components compared to simpler models, but the improvement in fit is minimal. 
- The AIC and BIC values are slightly higher, and the standard errors for the AR(1) and MA(1) coefficients remain relatively high. 
- Overall, the simpler ARIMA(1,0,1) or ARIMA(0,0,1) models might be preferred for their comparable fit and simplicity.

#### SARIMA Model

```{r}
# Fit SARIMA model
sarima_model <- auto.arima(ts_data)
summary(sarima_model)
```


- The SARIMA(0,0,0)(0,0,1)[12] model introduces a seasonal component to the ARIMA modeling, capturing any seasonal effects present in the data. 
- While the AIC and BIC values are competitive with other models, the simpler ARIMA models (like ARIMA(1,0,1) or ARIMA(0,0,1)) might still be preferred due to their simplicity and slightly better fit metrics.
- The seasonal term in the SARIMA model captures the seasonal variation but does not significantly improve the fit compared to non-seasonal ARIMA models.

#### Model Comparison

```{r}
# Comparing Models using AIC and BIC
models <- list(ar_model, ma_model, arma_model1, arma_model2, sarima_model)
model_names <- c("AR", "MA", "ARMA1", "ARMA2", "SARIMA(0,0,0)(0,0,1)[12] with Non-Zero Mean")

aic_values <- sapply(models, AIC)
bic_values <- sapply(models, BIC)

comparison <- data.frame(Model=model_names, AIC=aic_values, BIC=bic_values)
print(comparison)
```



### ACF, PACF, and EACF Plots

```{r}
# Plot Residuals of the SARIMA Model
acf_residuals <- residuals(sarima_model)
par(mar=c(5, 5, 4, 2) + 0.1)
acf(acf_residuals, main="ACF of SARIMA Model Residuals")
pacf(acf_residuals, main="PACF of SARIMA Model Residuals")
eacf(acf_residuals)
```


- The ACF and PACF plots suggest that the SARIMA model residuals are uncorrelated and resemble white noise.
- This indicates a good model fit as the model has successfully captured the time series patterns and structure.
- The lack of significant autocorrelation in the residuals validates the SARIMA model, confirming that it is a suitable model for the given data.


### Forecasting

```{r}
# Forecasting with the SARIMA model
sarima_forecast <- forecast(sarima_model, h=12)
plot(sarima_forecast, main="Forecasts from SARIMA(0,0,0)(0,0,1)[12] with Non-Zero Mean")
```


- The plot shows the forecasted values from the SARIMA(0,0,0)(0,0,1)[12] model with a non-zero mean.
- The confidence intervals widen as we move further into the future, reflecting increasing uncertainty in the predictions.
- The SARIMA model seems to capture the overall trend and seasonality in the data, but it may not fully capture the high volatility seen in the historical data.
- The residuals' ACF and PACF plots indicate that the model's residuals are mostly uncorrelated, suggesting a good fit.
- While the SARIMA model captures the seasonality and general trend, it may not fully account for the extreme fluctuations seen in the historical data. For better predictions, it might be necessary to explore other models or include additional explanatory variables.























